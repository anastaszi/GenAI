{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anastaszi/GenAI/blob/main/Fine_Tuning_LLMs_and_Embedding_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59e1a559",
      "metadata": {
        "id": "59e1a559"
      },
      "source": [
        "# Embedding Models + LLMs\n",
        "\n",
        "As we saw in the previous notebooks, embedding models can be very useful when mapping natural language to vectors that are used by downstream LLMs. When it comes to fine-tuning a pipline, since pipelines often include mulitple models, several different models can be fine tuned to better account for the nuances in your data. LLMs may leverage pre-trained word embeddings as part of their input or initialization, allowing them to benefit from the semantic information captured by the embedding models. The embedding models provide a foundation for understanding the meanings and relationships of individual words, which LLMs can build upon to generate coherent and contextually appropriate text.\n",
        "\n",
        "\n",
        "## Why Fine Tune an Embedding Model\n",
        "For example, say you're working with very specific legal text - text that's very different than what any embedding model is trained on. If we were to use an embedding model out of the box, we'd likley lose the nuianced context in the legal data. In order to avoid that, we could instead fine-tune the last seveal layes of our embedding model to betted account for the context. Similar to CNNs for imgage recongnition, the lower layers of embedding model are good a learning general patters with words - things like parts of speech, basic syntax, basic grammar etc. Since the higher layers in the network learn much more contextual and task-specific information - things like contextualized representations, semantic relationships, etc. If we fine tune those layers, we can efficiently use computer to keep the general relationships learned by lower layers, while customoizing the higher layers to our task and context.\n",
        "\n",
        "## Existing Solutions\n",
        "Fine-tuning embedding models isn't breaking news, insitutions have been doing it for years and releasing the models to the public for consumption. It's always best to take a look a see if there is an existing model out there that's traing on a similar task for similar text. For example, [here](https://huggingface.co/ipuneetrathore/bert-base-cased-finetuned-finBERT) is a HuggingFace link to a fine tuned version of BERT\n",
        "\n",
        "\n",
        "### Hardware Considerations\n",
        "Often times in fine tune embedding models, the reserach and documentation will refer to using a GPU to fine tune. This is because often times fine-tuning implies using large amounts of data. Depending on data size and model size it's ceratinly possible to fine tune on CPUs. We'll use small data in the exmaple below, and a CPU will provide more than enough computing power.\n",
        "\n",
        "\n",
        "\n",
        "Below example is adopted from [here](https://huggingface.co/blog/how-to-train-sentence-transformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b5d8128",
      "metadata": {
        "id": "5b5d8128"
      },
      "source": [
        "We'll be using a Hugginface sample dataset, so we'll need to ensure the `datasets` library is installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53e2818f",
      "metadata": {
        "id": "53e2818f",
        "outputId": "97eeb5af-9eeb-4e02-f9e2-dcf31d26e3a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: datasets in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (2.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from datasets) (1.23.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from datasets) (12.0.1)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from datasets) (1.4.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from datasets) (0.14.1)\n",
            "Requirement already satisfied: packaging in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from requests>=2.19.0->datasets) (1.26.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from pandas->datasets) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.15.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4479aa58",
      "metadata": {
        "id": "4479aa58"
      },
      "source": [
        "We'll be fine tuning a model from the `SentenceTransformer` library, namely the `distilroberta-base` model. `distilroberta-base` is a variant of the RoBERTa (Robustly Optimized BERT) model, which itself is based on the Transformer architecture. \"distil\" in the name stands for \"distillation\" and indicates that this model is a distilled version of the original RoBERTa model, aimed at being smaller and faster while maintaining a similar performance to the larger model.\n",
        "\n",
        "The distilroberta-base model follows the Transformer architecture, which includes stacked self-attention layers and feed-forward neural networks. It consists of multiple layers, each having a certain number of attention heads and hidden units.\n",
        "\n",
        "The \"base\" in the model name suggests that it is one of the base configurations available for the RoBERTa model. It has a smaller number of parameters compared to larger variants of the RoBERTa model, making it more lightweight.\n",
        "\n",
        "Like other transformer-based models, `distilroberta-base` uses a subword tokenization technique called Byte-Pair Encoding (BPE), which breaks down words into subword units to handle out-of-vocabulary words and improve generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1c910b9",
      "metadata": {
        "id": "c1c910b9",
        "outputId": "496628e1-d568-49a2-d88d-3204e8bbaa74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, models\n",
        "\n",
        "## Step 1: use an existing language model\n",
        "word_embedding_model = models.Transformer('distilroberta-base')\n",
        "\n",
        "## Step 2: use a pool function over the token embeddings\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "\n",
        "## Join steps 1 and 2 using the modules argument\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f261a0ac",
      "metadata": {
        "id": "f261a0ac"
      },
      "source": [
        "We'll use a dataset that's built into `HuggingFace` - the `QQP_triplets` dataset. This dataset contains content from the `Quora` website. Each example is a dictionary with three keys (query, pos, and neg) containing a list each (triplets). The first key contains an anchor sentence, the second a positive sentence, and the third a list of negative sentences.\n",
        "\n",
        "```\n",
        "{\"query\": [anchor], \"pos\": [positive], \"neg\": [negative1, negative2, ..., negativeN]}\n",
        "{\"query\": [anchor], \"pos\": [positive], \"neg\": [negative1, negative2, ..., negativeN]}\n",
        "...\n",
        "{\"query\": [anchor], \"pos\": [positive], \"neg\": [negative1, negative2, ..., negativeN]}\n",
        "```\n",
        "\n",
        "Note that this dataset is organized around semantic meaning - because of this we can use this to train models of semantic equivalence based on the true values of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7502df39",
      "metadata": {
        "id": "7502df39",
        "outputId": "6b51e7f1-9853-46f3-b58f-eb34beadc3b5",
        "colab": {
          "referenced_widgets": [
            "d329e7e54b2d4fe0951e04df7ac2e37e"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset json (/Users/marymoesta/.cache/huggingface/datasets/embedding-data___json/embedding-data--QQP_triplets-1f161ec5c28ee86f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d329e7e54b2d4fe0951e04df7ac2e37e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_id = \"embedding-data/QQP_triplets\"\n",
        "dataset = load_dataset(dataset_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "790b95ce",
      "metadata": {
        "id": "790b95ce"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f764ce0",
      "metadata": {
        "id": "7f764ce0"
      },
      "source": [
        "Let's bring out some basic stats about the Quora data we're using for fine-tuning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59ec76ef",
      "metadata": {
        "id": "59ec76ef",
        "outputId": "77460c48-47a5-464b-b319-0fc659f7cb3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- The embedding-data/QQP_triplets dataset has 101762 examples.\n",
            "- Each example is a <class 'dict'> with a <class 'dict'> as value.\n",
            "- Examples look like this: {'set': {'query': 'Why in India do we not have one on one political debate as in USA?', 'pos': ['Why cant we have a public debate between politicians in India like the one in US?'], 'neg': ['Can people on Quora stop India Pakistan debate? We are sick and tired seeing this everyday in bulk?', 'Why do politicians, instead of having a decent debate on issues going in and around the world, end up fighting always?', 'Can educated politicians make a difference in India?', 'What are some unusual aspects about politics and government in India?', 'What is debate?', 'Why does civic public communication and discourse seem so hollow in modern India?', 'What is a Parliamentary debate?', \"Why do we always have two candidates at the U.S. presidential debate. yet the ballot has about 7 candidates? Isn't that a misrepresentation of democracy?\", 'Why is civic public communication and discourse so hollow in modern India?', \"Aren't the Presidential debates teaching our whole country terrible communication skills and why is deliberate misrepresentation even allowed?\", 'Why are most Indian politicians uneducated?', 'Does Indian political leaders capable of doing face to face debates while running for office?', 'What is wrong with the Indian political system and the environment it has built in connection with the people of India? Have parties divided people more?', 'What is a debate?', 'Why do we have legislative council in india?', 'Why does the office of president of India, being politically neutral, not ask for Population control in India?', \"Why don't we discuss tax and foreign policies more in Indian elections but are instead obsessed with socialist schemes?\", 'Why do Indian politicians lack nationalist thinking?', 'Do you hate indian politicians?', 'Is India facing more stessful times and politically charged atmosphere when compared to Congress regime?', 'Who is the best politician in India? Why?', \"We all know about the present condition of Indian politicians; they are all just using us to run their train, but still, they win elections and rule over us. Why aren't people giving their vote to NOTA?\", 'Who are clean politicians in India?', 'Why are you not believing in Democracy of India?', 'What does politics in India mean? What are they actually doing?', 'What are the strongest arguments for a debate in favour of brain drain in India and what sources must be used for making a good short speech?', 'Do we really need an election commission in India?', 'Why is there no concept of political correctness in India? Is it a good thing or a bad thing?', 'Why is population control not on agenda of any political party in India?', 'Who are some of the most dangerous or worst politicians in India?']}}\n"
          ]
        }
      ],
      "source": [
        "print(f\"- The {dataset_id} dataset has {dataset['train'].num_rows} examples.\")\n",
        "print(f\"- Each example is a {type(dataset['train'][0])} with a {type(dataset['train'][0]['set'])} as value.\")\n",
        "print(f\"- Examples look like this: {dataset['train'][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca19a563",
      "metadata": {
        "id": "ca19a563"
      },
      "source": [
        "As mentioned above, fine-tuning can be costly and a timely operation. In an effort to speed things up a bit for this course, we'll only use have of the data available. In the code below, we'll loop through the `num_examples` and appended them to a `train_examples` list for model fine tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad9dcdf7",
      "metadata": {
        "id": "ad9dcdf7"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import InputExample\n",
        "\n",
        "train_examples = []\n",
        "train_data = dataset['train']['set']\n",
        "# For agility we only 1/2 of our available data\n",
        "n_examples = dataset['train'].num_rows // 2\n",
        "\n",
        "for i in range(n_examples):\n",
        "  example = train_data[i]\n",
        "  train_examples.append(InputExample(texts=[example['query'], example['pos'][0], example['neg'][0]]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "798e3879",
      "metadata": {
        "id": "798e3879"
      },
      "source": [
        "We'll use `PyTorch` as the training framework - users can also use `Tensorflow` if they prefer - but with `PyTorch`, a `DataLoader` object is expected in the `.fit` call. We'll instantiate the `DataLoader`, setting `shuffle` to `True` indicating data will be shuffled before each epoch and teh `batch_size` to 16 indicating each `batch` will contain 16 records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cee0c75b",
      "metadata": {
        "id": "cee0c75b"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "068c54ba",
      "metadata": {
        "id": "068c54ba"
      },
      "source": [
        "Triplet loss is a loss function used in training neural networks for learning embeddings, particularly in tasks involving similarity or distance learning. It is commonly used in tasks like face recognition, image retrieval, and information retrieval, where the goal is to learn representations that can accurately measure the similarity or dissimilarity between data points.\n",
        "\n",
        "The basic idea behind triplet loss is to encourage the neural network to map similar data points closer together in the embedding space and push dissimilar data points farther apart. The loss is computed using triplets of data points: an anchor point, a positive example (similar to the anchor), and a negative example (dissimilar to the anchor).\n",
        "\n",
        "The goal of triplet loss is to minimize the loss value by adjusting the model's parameters during training. This encourages the embeddings of the anchor and positive examples to be closer than the embeddings of the anchor and negative examples by at least the specified margin.\n",
        "\n",
        "By optimizing the triplet loss, the model learns to map similar data points closer together and separate dissimilar data points in the embedding space, enabling better similarity measurements and more effective retrieval or recognition tasks. However, collecting suitable triplets (i.e., anchor, positive, and negative examples) for training can be challenging and crucial for the success of triplet loss-based learning. Note that triplet loss is primarily used for tasks involving similarity or distance learning in embedding spaces (and sometimes image spaces)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "365bb18e",
      "metadata": {
        "id": "365bb18e"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import losses\n",
        "\n",
        "train_loss = losses.TripletLoss(model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef1f8b03",
      "metadata": {
        "id": "ef1f8b03"
      },
      "source": [
        "Let's not fit the model - note we are only training for 1 epoch for the sake of time and compute resources. If you have the time and resources, you can try training fo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78fbbd12",
      "metadata": {
        "id": "78fbbd12",
        "outputId": "7d43a87b-c24d-4a92-e68f-b7c1fc9990a0",
        "colab": {
          "referenced_widgets": [
            "844d3a90799e431a8e1817c3fc0d7072",
            "ccc96844edde4f27a601385d35097f43"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "844d3a90799e431a8e1817c3fc0d7072",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccc96844edde4f27a601385d35097f43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Iteration:   0%|          | 0/3181 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f3c14b6",
      "metadata": {
        "id": "6f3c14b6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}