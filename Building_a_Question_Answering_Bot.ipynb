{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anastaszi/GenAI/blob/main/Building_a_Question_Answering_Bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cc7bfe0",
      "metadata": {
        "id": "7cc7bfe0"
      },
      "source": [
        "# Building a Q&A Bot\n",
        "In the previous two notebooks, we've discussed quite a few concepts as they pertain to LLM basics and prompting. However, there hasn't been too much code to see how these things can be put together. In this notebook, we'll code through and example of a Question and Answering Bot, based on a fixed knowledge base. We'll cover the details of LLMs and prompting play into the patterns of search, retrevial, and text-generation in the application.\n",
        "\n",
        "To understand the details, we'll first take a look at the high level architecture of the application:\n",
        "\n",
        "![serach_retrevial_generation](./images/search_retrevial_generation.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1832fe7",
      "metadata": {
        "id": "a1832fe7"
      },
      "source": [
        "## Search\n",
        "\n",
        "Vector search could be an entire course on it's own - it's a vast topic with several nuanced details. However, for this course, we'll cover the basics. Vector search is a technique used to retrieve similar items or entities based on their vector representations. In vector search, data points are represented as vectors in a high-dimensional space, where each dimension corresponds to a specific feature or attribute. The goal is to efficiently search and retrieve items that are close or similar to a given query vector.\n",
        "\n",
        "Here are the basic steps involved in vector search:\n",
        "\n",
        "1. Vector Representation: Each item or entity in the search space is transformed into a vector representation. This can be done using techniques like word embeddings, document embeddings, or deep learning models. The vectors capture the essential characteristics or semantic meaning of the items.\n",
        "\n",
        "2. Indexing: The vector representations of the items are stored in an index, which organizes the vectors for efficient search. Various indexing structures like k-d trees, ball trees, or approximate nearest neighbor (ANN) indexes can be used to speed up the search process.\n",
        "\n",
        "3. Querying: When a search query is provided, it is also transformed into a vector representation using the same method used for the indexed items. The query vector represents the desired item or the characteristics being searched for.\n",
        "\n",
        "4. Similarity Measurement: The similarity between the query vector and the vectors in the index is calculated using a distance or similarity metric, such as Euclidean distance or cosine similarity. This metric quantifies the similarity or dissimilarity between two vectors based on their positions in the high-dimensional space.\n",
        "\n",
        "5. Retrieval: The items in the index that are most similar to the query vector are retrieved based on their proximity in the vector space. The retrieval can be performed using algorithms that efficiently search the index structure for nearest neighbors or approximate nearest neighbors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7ecceca",
      "metadata": {
        "id": "e7ecceca"
      },
      "source": [
        "## Vector Search Technologies\n",
        "\n",
        "There are various types of vector search technologies include vector libraries, vector databases and vector plugins.\n",
        "\n",
        "- **Vector Libraries**: A vector search library is specifically designed to handle large-scale vector data sets and perform search operations to find nearest neighbors or similar items to a given query vector. It utilizes advanced indexing structures and search algorithms to optimize the search process and provide fast retrieval of similar vectors. These libraries typically offer a set of APIs and functions that allow developers to build applications or systems that require similarity search capabilities. Popular vector search libraries include Annoy, FAISS, NMSLIB, and SPTAG. These libraries provide efficient indexing structures, search algorithms, and APIs to support similarity search tasks in various domains, including recommendation systems, content-based image retrieval, natural language processing, and data mining.\n",
        "\n",
        "- **Vector Databases**: A vector database, also known as a vector storage or vector index, is a specialized database system designed to efficiently store, manage, and retrieve vector data. It is specifically optimized for handling large-scale vector datasets and performing similarity searches on those vectors. While there are several overlapping capabilites between a vector database and a vector library, a vector database often includes all of those in a vector library and also include classic database features (CRUD operations, metadata handling, etc.). Some noteable vector databases are Chroma, Pinecone, Weavite, etc.\n",
        "\n",
        "-  **Vector Plugins**: The term \"vector plugins\" does not have a widely recognized or standard definition in the context of computer science or software development. However, based on the general understanding of plugins and vectors, we can interpret \"vector plugins\" as plugins or extensions specifically designed to enhance or expand the functionality related to vector processing, vector math, or vector-based operations in software applications. Vector plugins provide support for traditional database technologies (Postgres, mySQL, Mongo, etc.) to do vector search. This allows you to repurpose existing database technologies for vector search.\n",
        "\n",
        "\n",
        "Since this example includes a small amount of data, we'll use an Open Source vector database, Chroma, to index our data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63747da0",
      "metadata": {
        "id": "63747da0"
      },
      "source": [
        "For this example, we'll be using some state of the union text data and the `Langchain` loaders to load in our `.txt` file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81a5eecc",
      "metadata": {
        "id": "81a5eecc",
        "outputId": "c52417f4-e782-4720-8189-01d9debe74c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sentence_transformers in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (2.2.2)\n",
            "Requirement already satisfied: chromadb in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (0.3.29)\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from sentence_transformers) (4.28.1)\n",
            "Requirement already satisfied: tqdm in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: torchvision in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from sentence_transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from sentence_transformers) (1.23.2)\n",
            "Requirement already satisfied: scikit-learn in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from sentence_transformers) (1.1.2)\n",
            "Requirement already satisfied: scipy in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from sentence_transformers) (1.9.0)\n",
            "Requirement already satisfied: nltk in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from sentence_transformers) (0.14.1)\n",
            "Requirement already satisfied: pandas>=1.3 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from chromadb) (1.4.3)\n",
            "Requirement already satisfied: requests>=2.28 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from chromadb) (2.28.1)\n",
            "Requirement already satisfied: pydantic<2.0,>=1.9 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from chromadb) (1.10.7)\n",
            "Requirement already satisfied: hnswlib>=0.7 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from chromadb) (0.7.0)\n",
            "Requirement already satisfied: clickhouse-connect>=0.5.7 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from chromadb) (0.6.6)\n",
            "Requirement already satisfied: duckdb>=0.7.1 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from chromadb) (0.8.1)\n",
            "Requirement already satisfied: fastapi==0.85.1 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from chromadb) (0.85.1)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from chromadb) (0.22.0)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from chromadb) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from chromadb) (4.5.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from chromadb) (3.2.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from chromadb) (1.15.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from chromadb) (0.13.3)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from chromadb) (7.3.1)\n",
            "Requirement already satisfied: graphlib-backport>=1.0.3 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from chromadb) (1.0.3)\n",
            "Requirement already satisfied: starlette==0.20.4 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from fastapi==0.85.1->chromadb) (0.20.4)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from starlette==0.20.4->fastapi==0.85.1->chromadb) (3.6.1)\n",
            "Requirement already satisfied: aiohttp in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: certifi in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.6.15)\n",
            "Requirement already satisfied: importlib-metadata in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from clickhouse-connect>=0.5.7->chromadb) (4.12.0)\n",
            "Requirement already satisfied: urllib3>=1.26 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from clickhouse-connect>=0.5.7->chromadb) (1.26.12)\n",
            "Requirement already satisfied: pytz in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.2.1)\n",
            "Requirement already satisfied: zstandard in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from clickhouse-connect>=0.5.7->chromadb) (0.21.0)\n",
            "Requirement already satisfied: lz4 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from clickhouse-connect>=0.5.7->chromadb) (4.3.2)\n",
            "Requirement already satisfied: filelock in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: coloredlogs in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb) (4.22.3)\n",
            "Requirement already satisfied: sympy in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb) (1.15.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from requests>=2.28->chromadb) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from requests>=2.28->chromadb) (3.3)\n",
            "Requirement already satisfied: networkx in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: regex!=2019.12.17 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.3.23)\n",
            "Requirement already satisfied: click>=7.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\n",
            "Requirement already satisfied: h11>=0.8 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: joblib in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from nltk->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from torchvision->sentence_transformers) (9.5.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from importlib-metadata->clickhouse-connect>=0.5.7->chromadb) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/marymoesta/Library/Python/3.8/lib/python/site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb) (1.2.0)\n",
            "Installing collected packages: openai\n",
            "\u001b[33m  WARNING: The script openai is installed in '/Users/marymoesta/Library/Python/3.8/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed openai-0.27.8\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install sentence_transformers chromadb openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66752a76",
      "metadata": {
        "id": "66752a76"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62db89be",
      "metadata": {
        "id": "62db89be",
        "outputId": "35f28c59-e8a3-4a5c-d1f4-52ec7125ff9c",
        "colab": {
          "referenced_widgets": [
            "1daa3e80d8f940a39a492a334ef554c0",
            "41b14fcf66f64aee8f821ca2dd6fdeb8",
            "74d00fb6783c42b5a6ee33e41510c11a",
            "708e1c3f701949b595946a5bc98d348e",
            "7061c653c44646dea770764130baa891",
            "57d6a273021c4ade86bf1f92252fc6fe",
            "8e7a0a56ce454eff857f30f9536412dc",
            "bd31c297e1334956831f227bf9d054a1",
            "a120bd8f75924044ad8b4bd7f24c2233",
            "1e916cc3b89a42cf8c769f00e116a417",
            "c37ce486f4d049cba76d78b061e1ae5f",
            "fc8ee83dd624400abb93a6939c70cf22",
            "cdce985211234c16b3428b89ae1d31ff",
            "6d380333d5c843e88799101f1c142cf9"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1daa3e80d8f940a39a492a334ef554c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)a8e1d/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41b14fcf66f64aee8f821ca2dd6fdeb8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74d00fb6783c42b5a6ee33e41510c11a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)b20bca8e1d/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "708e1c3f701949b595946a5bc98d348e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)0bca8e1d/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7061c653c44646dea770764130baa891",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57d6a273021c4ade86bf1f92252fc6fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)e1d/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e7a0a56ce454eff857f30f9536412dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd31c297e1334956831f227bf9d054a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a120bd8f75924044ad8b4bd7f24c2233",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e916cc3b89a42cf8c769f00e116a417",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)a8e1d/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c37ce486f4d049cba76d78b061e1ae5f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc8ee83dd624400abb93a6939c70cf22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)8e1d/train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdce985211234c16b3428b89ae1d31ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)b20bca8e1d/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d380333d5c843e88799101f1c142cf9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)bca8e1d/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "ValueError",
          "evalue": "Could not import chromadb python package. Please install it with `pip install chromadb`.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/langchain/vectorstores/chroma.py:70\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m texts \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n\u001b[1;32m      6\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings()\n\u001b[0;32m----> 7\u001b[0m docsearch \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m qa \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(llm\u001b[38;5;241m=\u001b[39mOpenAI(), chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m, retriever\u001b[38;5;241m=\u001b[39mdocsearch\u001b[38;5;241m.\u001b[39mas_retriever())\n",
            "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/langchain/vectorstores/chroma.py:489\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    488\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 489\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/langchain/vectorstores/chroma.py:450\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    432\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chroma_collection\n",
            "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/langchain/vectorstores/chroma.py:73\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import chromadb python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install chromadb`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m client\n",
            "\u001b[0;31mValueError\u001b[0m: Could not import chromadb python package. Please install it with `pip install chromadb`."
          ]
        }
      ],
      "source": [
        "# Load in our Text Data using TextLoader\n",
        "loader = TextLoader(\"./data/state_of_the_union.txt\")\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70321c8b",
      "metadata": {
        "id": "70321c8b"
      },
      "source": [
        "Now that we've read in our data, before we index embeddings in Chroma, we'll need to chunk the data. Chunking refers to the process of dividing a piece of text into smaller, manageable subsets or chunks. This technique is employed to improve the efficiency and scalability of similarity search operations, especially when dealing with massive or high-dimensional vector datasets. Ideal chunk size is entirely dependent on the number of documents, layout of those documents, context sharing across documents, the nature of queries, and the token limit of the chat model being used. Chunking best practices are still evolving, but [here](https://www.pinecone.io/learn/chunking-strategies/) is a good resource.\n",
        "\n",
        "Since our data is small, we can get away with using the default chunk size, with no chunk_overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a078cb6",
      "metadata": {
        "id": "2a078cb6"
      },
      "outputs": [],
      "source": [
        "# Chunk our data into smaller pieces for more effective vector search\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96aa694e",
      "metadata": {
        "id": "96aa694e"
      },
      "source": [
        "After data is chunked, we'll now use the `MPNetModel` from the `SentenceTransformer` library via `HuggingFace` to map our text data into numeric vectors and index them via the `from_documents` function in the `Chroma` module via `Langchain`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c805f3c",
      "metadata": {
        "id": "3c805f3c"
      },
      "outputs": [],
      "source": [
        "embeddings = HuggingFaceEmbeddings()\n",
        "docsearch = Chroma.from_documents(texts, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34481a1e",
      "metadata": {
        "id": "34481a1e"
      },
      "source": [
        "The `docsearch` object is an instance of a `Chroma` vector store in `Langchain`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3d13f2b",
      "metadata": {
        "id": "b3d13f2b",
        "outputId": "7fbc2a54-d2e8-458e-c5e0-ab3fafe4272c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "langchain.vectorstores.chroma.Chroma"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(docsearch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f22e550",
      "metadata": {
        "id": "5f22e550",
        "outputId": "71f47377-fa42-4a7c-d6e7-d4c2a7c4a8bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content=\"For example, Thomas Jefferson thought Washington's oral presentation was too kingly for the new republic. Likewise, Congress's practice of giving a courteous reply in person at the President's residence was too formal. Jefferson detailed his priorities in his first annual message in 1801 and sent copies of the written message to each house of Congress. The President's annual message, as it was then called, was not spoken by the President for the next 112 years. The message was often printed in full or as excerpts in newspapers for the American public to read.\\n\\nThe first President to revive Washington's spoken precedent was Woodrow Wilson in 1913. Although controversial at the time, Wilson delivered his first annual message in person to both houses of Congress and outlined his legislative priorities.\", metadata={'source': './data/state_of_the_union.txt'}),\n",
              " Document(page_content=\"Most annual messages outline the President's legislative agenda and national priorities in general or specific terms. James Monroe in 1823 discussed the centerpiece of his foreign policy, now-known as the Monroe Doctrine, which called on European countries to end western colonization. Lincoln famously expressed his desire for slave emancipation in 1862, and Franklin Roosevelt spoke about the now-famous four freedoms during his State of the Union message in 1941.\\n\\nWhatever the form, content, delivery method or broadcast medium, the President's annual address is a backdrop for national unity. The State of the Union gives the President an opportunity to reflect on the past while presenting his hopes for the future to Congress, the American people and the world.\", metadata={'source': './data/state_of_the_union.txt'}),\n",
              " Document(page_content='The President\\'s focus, however, was on the very concept of union itself. Washington and his administration were concerned with the challenges of establishing a nation and maintaining a union. The experiment of American democracy was in its infancy. Aware of the need to prove the success of the \"union of states,\" Washington included a significant detail in his speech. Instead of datelining his message with the name of the nation\\'s capital, New York, Washington emphasized unity by writing \"United States\" on the speech\\'s dateline.\\n\\nSince Washington\\'s first speech to Congress, U.S. Presidents have \"from time to time\" given Congress an assessment of the condition of the union. Presidents have used the opportunity to present their goals and agenda through broad ideas or specific details. The annual message or \"State of the Union\" message\\'s length, frequency, and method of delivery have varied from President to President and era to era.', metadata={'source': './data/state_of_the_union.txt'}),\n",
              " Document(page_content='From Time to Time: History of the State of the Union\\n\\n\"The President shall from time to time give to Congress information of the State of the Union and recommend to their Consideration such measures as he shall judge necessary and expedient.\" Article II, Sec. 3, U.S. Constitution\\n\\nGeorge Washington rode on a carriage driven by six horses from his house on Cherry Street to Federal Hall in New York to give his deliver his first annual message in person.\\nOn a cold January morning, the President rode in a carriage drawn by six horses from his residence on Cherry Street in New York to Federal Hall for a joint meeting of the two bodies of Congress, the House of Representatives and the Senate. When George Washington personally delivered the first annual message to Congress on January 8, 1790, he was aware of his constitutional duty to deliver his message and of the precedent he was setting for future presidents.', metadata={'source': './data/state_of_the_union.txt'})]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example similarity search\n",
        "docsearch.similarity_search(\"Thomas Jefferson\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a8fa893",
      "metadata": {
        "id": "7a8fa893"
      },
      "source": [
        "To review the architecture above:\n",
        "\n",
        "![search_retrevial_generation](./images/search_retrevial_generation.png)\n",
        "\n",
        "\n",
        "So far we've covered the first few pieces :\n",
        "- Search: embedded and indexed our knolwedge base using `TextLoader`, `HuggingFaceEmbeddings`, and `Chroma`\n",
        "- Retrieval: within the `Chroma` collection, we can leveage the built in search capabilites to identify relevent piece of content based on the query\n",
        "\n",
        "\n",
        "Let's shift the focus to the text generation portion:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb6c9299",
      "metadata": {
        "id": "cb6c9299"
      },
      "source": [
        "## Langchain Chains\n",
        "Chains are an incredible versitile part of the `Langchain` library. Chains allow for users to build more complex applications by chaining several steps and models together into pipelines. Chains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components.\n",
        "\n",
        "Read more about Chains [here](https://python.langchain.com/docs/modules/chains/)\n",
        "\n",
        "\n",
        "`Langchain` is beind so rapidly developed that new, built-in chains for different applications are popping up within a few weeks. `Langchain` recently released a `RetrevialQA` chain that is designed to be used in a Q&A Bost application. This chain takes the following inputs:\n",
        "- **llm**: large language model used for text generation\n",
        "- **retrevier**: this is the retrevier object\n",
        "- **chain_type**: this denotes how relevant content is passed into the llm, more details [here](https://python.langchain.com/docs/modules/chains/popular/vector_db_qa#chain-type)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Note that the `RetrievalQA` chain below uses an `OpenAI` chat model. In order to run this code, you'll need to generate an `OpenAI` key and store it as an environment variable:\n",
        "\n",
        "```import os\n",
        "    os.environ[\"OPENAI_API_KEY\"] = INSERT_KEY\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dee181a",
      "metadata": {
        "id": "2dee181a"
      },
      "outputs": [],
      "source": [
        "# Create the chain\n",
        "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "499a1265",
      "metadata": {
        "id": "499a1265",
        "outputId": "c100df0d-bc52-437e-a6e8-88caec1148ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" Thomas Jefferson thought Washington's oral presentation was too kingly for the new republic and Congress's practice of giving a courteous reply in person at the President's residence was too formal.\""
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Prompt the chain with a question\n",
        "qa.run(\"What did the president say about Thomas Jefferson?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "016b585a",
      "metadata": {
        "id": "016b585a"
      },
      "source": [
        "Great! We've been able to stand up a working example of a search, retrieval, and text generation process. Let's add one more step of complexity, by adding in a prompt template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a0749ef",
      "metadata": {
        "id": "3a0749ef"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer in Iambic Pentameter:\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08983dc2",
      "metadata": {
        "id": "08983dc2"
      },
      "outputs": [],
      "source": [
        "# Incorporate the prompt into the chain\n",
        "chain_type_kwargs = {\"prompt\": prompt}\n",
        "\n",
        "qa_with_prompt = RetrievalQA.from_chain_type(llm=OpenAI(),\n",
        "                                             chain_type=\"stuff\",\n",
        "                                             retriever=docsearch.as_retriever(),\n",
        "                                             chain_type_kwargs=chain_type_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b799d101",
      "metadata": {
        "id": "b799d101",
        "outputId": "2fbff3fb-3224-4636-fef2-0500f2e70621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "George Washington's speech did set/A precedent for all presidents yet/For kingly presence he was seen/And courteous replies at his home were keen/He wrote a dateline of United States/To prove the nation's union and fate.\n"
          ]
        }
      ],
      "source": [
        "print(qa_with_prompt.run(\"What did the president say about George Washington?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d56151bb",
      "metadata": {
        "id": "d56151bb"
      },
      "source": [
        "Fantastic! We've now been able to incorporate some of the prompt engineering best practices and techniques into our search, retrevial, and text generation pipeline."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}