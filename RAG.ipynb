{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anastaszi/GenAI/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "617e5b24-30d3-4fe5-a559-1e8ece08205a",
      "metadata": {
        "id": "617e5b24-30d3-4fe5-a559-1e8ece08205a"
      },
      "source": [
        "# Retreival Augmented Generation\n",
        "Retrieval-Augmented Generation, abbreviated RAG, is an architecture used in natural language processing (NLP) that combines the strengths of both retrieval and generation models to perform various language understanding and generation tasks.\n",
        "\n",
        "RAG applications typically consist of a few different components:\n",
        "\n",
        "1. **Retrieval Component**: The retrieval component is responsible for searching a large database of text to find relevant information. It usually employs an information retrieval (IR) system or a pre-trained retrieval model like a dense retriever, which ranks documents or passages based on their relevance to a given query.\n",
        "\n",
        "2. **Generation Component**: The generation component is a language model, often a variant of the Transformer architecture, that can generate human-like text. It is capable of taking retrieved passages or documents and generating coherent and contextually relevant responses or completions.\n",
        "\n",
        "3. **Interaction**: RAG models combine these two components in a way that allows them to interact. Typically, the retrieved passages or documents serve as additional context for the generation model. This context can help the generation model produce more informed and contextually relevant responses.\n",
        "\n",
        "4. **Applications**: RAG models are versatile and can be used for a wide range of NLP tasks, including question answering, text summarization, chatbots, and more. For instance, in question answering, the retrieval component can identify relevant passages containing answers, and the generation component can generate concise and accurate responses based on that context.\n",
        "\n",
        "5. **Fine-Tuning**: RAG models are often fine-tuned on task-specific data to optimize their performance for a particular application.\n",
        "\n",
        "6. **Efficiency**: RAG models can be more efficient than traditional approaches for some tasks, especially when dealing with large-scale document collections. By narrowing down the search space with retrieval, they can reduce the computational burden on the generation model.\n",
        "\n",
        "RAG is particularly valuable when dealing with tasks that require access to external knowledge sources, such as open-domain question answering. It allows models to retrieve relevant information from a vast corpus of text and use that information to generate coherent and contextually appropriate responses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9409de9-37dd-410e-b3b1-8c6c101a280b",
      "metadata": {
        "id": "c9409de9-37dd-410e-b3b1-8c6c101a280b"
      },
      "source": [
        "## RAG vs. Fine-Tuning\n",
        "In general, RAG is a means towards tailoring more specifically to a subset of content. Fine-tuning is another method aimed at doing this, however the requirements on fine-tuning are often both compute intensive and labeled data intensive. RAG offers an alternative to fine-tuning that doens't require extensive compute and labeled data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00e1137e-81d6-4a66-be7b-cb2dcfb6d2ce",
      "metadata": {
        "id": "00e1137e-81d6-4a66-be7b-cb2dcfb6d2ce"
      },
      "source": [
        "Before we get started with this tutorial, we'll need to install a few additional libraries (if not installed already)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "302b667a-a323-4ffd-8cae-c50a37438997",
      "metadata": {
        "id": "302b667a-a323-4ffd-8cae-c50a37438997"
      },
      "outputs": [],
      "source": [
        "%pip install langchain pypdf openai chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0795da3f-187e-441b-9122-b061c768052c",
      "metadata": {
        "id": "0795da3f-187e-441b-9122-b061c768052c"
      },
      "source": [
        "For this tutorial, we'll be using the `cnn_dailymail` dataset from `HuggingFace`. The CNN / DailyMail Dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. The current version supports both extractive and abstractive summarization, though the original version was created for machine reading and comprehension and abstractive question answering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf97d050-43da-407a-a88f-706f4720253c",
      "metadata": {
        "id": "bf97d050-43da-407a-a88f-706f4720253c"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "# Load the dataset from Huggingface\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "# Visually inspect\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b0bf25c-bff6-487e-a04b-9c7f27cf3b2e",
      "metadata": {
        "id": "7b0bf25c-bff6-487e-a04b-9c7f27cf3b2e"
      },
      "outputs": [],
      "source": [
        "# Limit to 50 rows as a sample\n",
        "filtered_pdf = dataset[\"train\"].to_pandas().head(50)\n",
        "\n",
        "# Print out the articles to visually inspect\n",
        "for i in filtered_pdf[\"article\"]:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fb3fb7c-8bfd-4e5d-a7dd-62ba550c3a86",
      "metadata": {
        "id": "1fb3fb7c-8bfd-4e5d-a7dd-62ba550c3a86"
      },
      "source": [
        "This demo will be using OpenAI for both embeddings and for a text generation model. In order to use these models, a paid token needs to be configured:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b7970df-3d77-4791-9773-aff085f4e093",
      "metadata": {
        "id": "0b7970df-3d77-4791-9773-aff085f4e093"
      },
      "outputs": [],
      "source": [
        "# Configure OpenAI key\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "310b616b-d8af-49bb-8828-a39ac82f61ea",
      "metadata": {
        "id": "310b616b-d8af-49bb-8828-a39ac82f61ea"
      },
      "source": [
        "## Indexing Content\n",
        "Indexing content in a Retrieval Question-Answering (QA) chain for Large Language Models (LLMs) is a crucial step in the pipeline to enable efficient and accurate information retrieval. Retrieval QA chains combine the strengths of both retrieval-based and generation-based approaches to answer questions or provide information. Here's an overview of the concept:\n",
        "\n",
        "1. **Retrieval Component**:\n",
        "   \n",
        "   - In a Retrieval QA chain, the process begins with the retrieval component. This component is responsible for searching and identifying relevant documents, passages, or content from a large corpus of data. It doesn't generate answers directly but narrows down the search space.\n",
        "\n",
        "   - **Indexing**: The first step in indexing content involves creating an index of the documents or passages that the QA system will search. This index is designed for efficient and fast lookup based on specific retrieval criteria.\n",
        "\n",
        "   - **Preprocessing**: Text data within documents is preprocessed to improve retrieval efficiency. Common preprocessing steps include tokenization, stemming, removing stop words, and encoding text into numerical representations like embeddings.\n",
        "\n",
        "   - **Vector Embeddings**: Many modern retrieval systems use vector embeddings to represent documents or passages. Each document is transformed into a dense vector in a high-dimensional space. This allows for efficient similarity computations between queries and documents.\n",
        "\n",
        "2. **Question Representation**:\n",
        "\n",
        "   - The next step involves representing the user's question as a query. The question is tokenized and transformed into a vector representation similar to the documents in the index. This vector is used to compare the question to the documents in the retrieval index.\n",
        "\n",
        "3. **Scoring and Ranking**:\n",
        "\n",
        "   - Once the question is represented as a vector, a similarity score is computed between the question vector and the vectors of indexed documents. Common similarity measures include cosine similarity or dot product.\n",
        "\n",
        "   - Documents are ranked based on their similarity scores to the question. The most similar documents are considered as potential candidates for answering the question.\n",
        "\n",
        "4. **Passage Selection**:\n",
        "\n",
        "   - After ranking the documents, the retrieval system may select specific passages or segments within the documents that are most likely to contain the answer. This step helps in reducing the amount of text that needs to be processed by the generation component.\n",
        "\n",
        "The key advantage of this Retrieval QA chain is that it combines the precision of retrieval-based systems (which are good at finding relevant documents) with the flexibility and language understanding of generation-based models (which are good at generating human-like answers). By indexing content efficiently and selecting relevant passages, the retrieval component narrows down the search space for the generation component, resulting in faster and more accurate responses to user queries.\n",
        "\n",
        "Overall, indexing content is a critical step in enabling efficient and effective question-answering systems powered by LLMs, particularly in scenarios where the information to be retrieved is extensive and diverse."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f001992-0730-4285-a799-afd85a43e24b",
      "metadata": {
        "id": "6f001992-0730-4285-a799-afd85a43e24b"
      },
      "source": [
        "In order to index the `cnn_dailymail` content, we'll need to first create a list of the content to be indexed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a47751e3-cb0f-4146-9f87-dc64e7613b81",
      "metadata": {
        "id": "a47751e3-cb0f-4146-9f87-dc64e7613b81"
      },
      "outputs": [],
      "source": [
        "# Pre-process the text to be able to load it into Chroma\n",
        "article_content = filtered_pdf[\"article\"].to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17875d7c-7812-4ee4-8c89-6e049a2ecec3",
      "metadata": {
        "id": "17875d7c-7812-4ee4-8c89-6e049a2ecec3"
      },
      "source": [
        "### `Chroma`\n",
        "`Chroma` is an light-weight, open source vector database. A vector database, also known as a vector store or vector database management system, is a specialized type of database designed to efficiently store, manage, and query high-dimensional vector data. In vector databases, data is represented as vectors of numerical values, and the primary focus is on similarity search, retrieval, and analysis of this vector data. These databases are commonly used in applications where similarity or distance measurements between data points are essential, such as content-based recommendation systems, image similarity search, natural language processing (NLP), and more.\n",
        "\n",
        "Here are some key characteristics and features of vector databases:\n",
        "\n",
        "1. **Vector Data Storage**: Vector databases are optimized for storing high-dimensional vector data efficiently. They use data structures and indexing methods that enable fast retrieval of vectors based on similarity or distance metrics.\n",
        "\n",
        "2. **Similarity Search**: The primary function of a vector database is to perform similarity search. Users can query the database with a vector, and the database returns vectors from the stored data that are most similar to the query vector based on a specified distance metric (e.g., cosine similarity, Euclidean distance).\n",
        "\n",
        "3. **Indexing**: Vector databases employ indexing techniques specifically designed for vector data. These indexing structures enable quick lookup and retrieval of vectors based on their properties. Examples of indexing methods include tree structures like Ball Trees or Approximate Nearest Neighbors (ANN) algorithms.\n",
        "\n",
        "4. **Scalability**: Many vector databases are designed to scale horizontally, making them suitable for handling large datasets and distributed deployments. This is crucial for applications that involve vast amounts of vector data.\n",
        "\n",
        "5. **Multimodal Data**: Some vector databases support multimodal data, meaning they can handle vectors representing different types of data, such as text, images, audio, or sensor data, in a unified manner.\n",
        "\n",
        "6. **Integration**: Vector databases are often integrated with other components of data pipelines, such as machine learning models or recommendation engines, to enable efficient similarity-based operations.\n",
        "\n",
        "7. **Dimensionality Reduction**: Some vector databases offer dimensionality reduction techniques like Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-SNE) to reduce the dimensionality of vector data for more efficient storage and querying.\n",
        "\n",
        "8. **Real-Time and Batch Processing**: Depending on the use case, vector databases can support real-time or batch processing of vector data, making them suitable for various applications, including real-time recommendations and offline analysis.\n",
        "\n",
        "Vector databases play a crucial role in many modern AI and data-driven applications that require the efficient retrieval and analysis of high-dimensional vector data. They enable content-based recommendations, similarity-based search, clustering, and other operations that rely on measuring similarity or distance between data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20065991-9c1a-4807-a2aa-ef084d93f38b",
      "metadata": {
        "id": "20065991-9c1a-4807-a2aa-ef084d93f38b"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Create the Index using Chroma\n",
        "chroma_index = Chroma.from_texts(article_content, OpenAIEmbeddings())\n",
        "\n",
        "# Test the index with a similarity serach\n",
        "docs = chroma_index.similarity_search(\"harry potter\", k=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dc07415-26ec-43bc-b9e0-b407bbd7dfcf",
      "metadata": {
        "id": "2dc07415-26ec-43bc-b9e0-b407bbd7dfcf"
      },
      "source": [
        "Take a look at the result of the test similarity search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bdf0479-2bff-4ac2-9c23-5749167303fd",
      "metadata": {
        "id": "8bdf0479-2bff-4ac2-9c23-5749167303fd"
      },
      "outputs": [],
      "source": [
        "# Inspect the returned page content\n",
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ba10ea8-5d9c-46f8-9bc9-11ccb631090e",
      "metadata": {
        "id": "4ba10ea8-5d9c-46f8-9bc9-11ccb631090e"
      },
      "source": [
        "## Configure the Chain\n",
        "Now that we've indexed all the revelant content, let's configure the chain. A chain is a concept in `Langchain`:\n",
        "\n",
        "```Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components.LangChain provides the Chain interface for such \"chained\" applications. We define a Chain very generically as a sequence of calls to components, which can include other chains.```\n",
        "\n",
        "These chains are very useful when stringing components together to create a cohesive end to end pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "927896a6-b662-4075-8b4a-f57dba7b5232",
      "metadata": {
        "id": "927896a6-b662-4075-8b4a-f57dba7b5232"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "chain = RetrievalQA.from_chain_type(OpenAI(), chain_type=\"stuff\", retriever=chroma_index.as_retriever())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b11f4872-63dd-4151-978f-ecda620c1d0b",
      "metadata": {
        "id": "b11f4872-63dd-4151-978f-ecda620c1d0b"
      },
      "source": [
        "## Query the Chain to test knowledge\n",
        "Now that the chain is configured, we can easily test it using a question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d9f127c-875f-4077-a0bc-0895cec1b2b9",
      "metadata": {
        "id": "2d9f127c-875f-4077-a0bc-0895cec1b2b9"
      },
      "outputs": [],
      "source": [
        "query = \"What's going on with the Harry Potter actor these days?\"\n",
        "\n",
        "chain.run(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09be84e7-3823-4076-b13f-fbf372a36e31",
      "metadata": {
        "id": "09be84e7-3823-4076-b13f-fbf372a36e31"
      },
      "source": [
        "## Enhancing the Chain with Prompt Engineering\n",
        "Now that we've validated that the extisting chain work with the LLM and the vectorized content, we can further discuss incorporating prompt templating. Using teh `RetrievalQAChain` from `LangChain` means that, up until this point, we are relying on the prompting mechansims built into that chain. However, it's clear that prompts play a large role on the relevancy of responses from these LLMs, so it's important to know how to use custom prompting if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c33ab71-99ed-4ff8-8077-5d4268af18e1",
      "metadata": {
        "id": "8c33ab71-99ed-4ff8-8077-5d4268af18e1"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts.chat import SystemMessage, HumanMessagePromptTemplate\n",
        "human_message = \"\"\"### Given the CONTEXT, answer the QUESTION.\n",
        "CONTEXT: {context}\n",
        "###\n",
        "QUESTION: {question}\n",
        "###\n",
        "\"\"\"\n",
        "\n",
        "system_message = \"\"\"You are a cheerful, helpful assistant that answers questions using the context given.\n",
        "If you cannot answer the question using the context, then just say `I don't know`\"\"\"\n",
        "\n",
        "template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(content=(system_message)),\n",
        "        HumanMessagePromptTemplate.from_template(human_message)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae4f53f8-3ca4-43ec-980b-11b94dbc62d7",
      "metadata": {
        "id": "ae4f53f8-3ca4-43ec-980b-11b94dbc62d7"
      },
      "source": [
        "See [here](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api) for some general guidelines of OpenAI prompting best practices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af35f293-6588-4abd-a854-7fb5fb12c751",
      "metadata": {
        "id": "af35f293-6588-4abd-a854-7fb5fb12c751"
      },
      "outputs": [],
      "source": [
        "# Pass the prompt template via the chain_types_kwargs parameter\n",
        "chain_type_kwargs = {\"prompt\": template}\n",
        "chain_with_prompting = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\",\n",
        "                                   retriever=chroma_index.as_retriever(), chain_type_kwargs=chain_type_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bef3f5c2-a7c4-4749-aa4b-075f8628e93d",
      "metadata": {
        "id": "bef3f5c2-a7c4-4749-aa4b-075f8628e93d"
      },
      "source": [
        "Test the new chain with prompt templating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79f3fd3e-3cbe-4f89-ae96-37ccb9705ac4",
      "metadata": {
        "id": "79f3fd3e-3cbe-4f89-ae96-37ccb9705ac4"
      },
      "outputs": [],
      "source": [
        "print(chain_with_prompting.run(query))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78bfdb9d-5f58-401a-aaaf-a12fbde8fe26",
      "metadata": {
        "id": "78bfdb9d-5f58-401a-aaaf-a12fbde8fe26"
      },
      "source": [
        "## Summary\n",
        "This tutorial serves as an informative and hands-on introduction to the powerful Retrieval-Augmented Generation (RAG) pipeline. Through a series of interactive examples, users will explore the fundamental components of RAG, showcasing its unique ability to combine retrieval-based and generation-based approaches in natural language processing. By working with real-world data, participants will learn how to set up a retrieval system, query and rank documents, and seamlessly integrate this contextual information into a generation model. This practical guide offers a comprehensive overview of the RAG architecture, enabling users to harness its capabilities for tasks such as question answering, content summarization, and more. Whether you're new to RAG or looking to deepen your understanding, this notebook provides a clear and hands-on demonstration of its core functionalities."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}