{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anastaszi/GenAI/blob/main/introduction_to_llms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8a16cff",
      "metadata": {
        "id": "c8a16cff"
      },
      "source": [
        "# Large Language Model Basics\n",
        "This notebook serves as a primer on large language models. This notebook will briefly discuss:\n",
        "- How LLMs are different from other language models\n",
        "- Why LLMs can be so powerful\n",
        "- A brief synopsis of the transformer architecture\n",
        "- An introduction to flow chaining with examples\n",
        "\n",
        "If you're already knowledgeable on the information above, feel free to skip this notebook. Additionally if you're looking for fundamentals of NLP as a whole, please reference the ODSC NLP course."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29f6bfc2",
      "metadata": {
        "id": "29f6bfc2"
      },
      "source": [
        "## How do LLMs Differ from Language Models?\n",
        "\n",
        "\n",
        "LLMs, or Large Language Models, differ from other language models primarily in their size and complexity. LLMs are characterized by their immense scale, with billions of parameters, which allows them to capture and learn from a vast amount of textual data. This large-scale training enables LLMs to generate more coherent and contextually relevant responses compared to smaller language models.\n",
        "\n",
        "Additionally, LLMs often employ advanced architectures, such as Transformer models, that utilize self-attention mechanisms to better understand the relationships between words and phrases in a given text. This architecture allows LLMs to capture long-range dependencies and generate more accurate and coherent outputs. See below for a visual representation of this architecture\n",
        "\n",
        "Another crucial aspect of LLMs is their training process. They are trained on large, diverse datasets, which helps them acquire a wide range of knowledge and linguistic patterns. This training process enables LLMs to perform well on various natural language processing tasks, including text completion, summarization, translation, and question answering.\n",
        "\n",
        "Overall, the significant differences between LLMs and other language models lie in their scale, complexity, and advanced architectures, which contribute to their improved performance in understanding and generating human-like text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "804682a7",
      "metadata": {
        "id": "804682a7"
      },
      "source": [
        "## Why are LLMs so Powerful?\n",
        "\n",
        "The increased power of LLMs compared to other language models primarily stems from their scale and the amount of training data they are exposed to. By training on vast amounts of diverse text data, LLMs acquire a broad understanding of language patterns, grammar, and semantics. This enables them to generate more contextually accurate and coherent responses.\n",
        "\n",
        "The large number of parameters in LLMs allows for a higher degree of complexity in modeling language. These models can capture intricate relationships between words and phrases, including long-range dependencies, which leads to more nuanced and sophisticated outputs. LLMs are capable of understanding context, making inferences, and generating text that closely resembles human-generated content.\n",
        "\n",
        "Furthermore, the sheer size of LLMs allows them to encapsulate a wealth of knowledge. They learn from a diverse range of sources, including books, articles, and websites, and can recall and apply that knowledge when generating responses. This broad knowledge base enhances their ability to provide informative and accurate answers across various domains.\n",
        "\n",
        "The power of LLMs also lies in their versatility. They can be fine-tuned on specific tasks or domains, which further enhances their performance and adaptability. This flexibility makes LLMs applicable to a wide range of applications, from chatbots and virtual assistants to content generation and language translation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02514e36",
      "metadata": {
        "id": "02514e36"
      },
      "source": [
        "## The Transformer Architecture\n",
        "\n",
        "The **Transformer architecture** and the **attention mechanism** are key components of modern language models, including LLMs.\n",
        "\n",
        "1. Transformer Architecture: The Transformer architecture is a neural network model that revolutionized natural language processing tasks. It was introduced in the paper [*Attention is All You Need*](https://arxiv.org/abs/1706.03762) in 2017. Unlike previous models that relied on recurrent or convolutional layers, the Transformer employs a purely attention-based approach. The core idea of the Transformer is to process entire sequences of words simultaneously rather than sequentially. It consists of two main components: an encoder and a decoder. The encoder takes an input sequence and processes it in parallel, encoding the information about the relationships between the words. The decoder then generates an output sequence based on the encoded input.\n",
        "\n",
        "2. Attention Mechanism: The attention mechanism is a fundamental component of the Transformer architecture. It allows the model to focus on specific parts of the input sequence while processing each word. The attention mechanism calculates attention weights for each word in the input sequence, indicating its relevance to the current word being processed. The attention mechanism works by computing the similarity between a target word and all the words in the input sequence. This similarity, often measured using dot product or other scoring functions, determines the importance or weight assigned to each word. The attention weights are then used to compute a weighted sum of the input words, which provides a context vector for the current word in the sequence.\n",
        "\n",
        "By incorporating the attention mechanism, the Transformer can capture long-range dependencies and give more importance to relevant words in the input sequence. This attention-based approach has proven highly effective in language modeling tasks, allowing models to generate more coherent and contextually relevant outputs. Something important to understand is that the transformer architecture has non-deterministic elements, which means there will always be some level of variability in models that leverage this architecture. This is why you could ask an LLM the same thing three times, and get three different answers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce843687",
      "metadata": {
        "id": "ce843687"
      },
      "source": [
        "![transformer_architecture](./images/transformer_architecture.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b36699b0",
      "metadata": {
        "id": "b36699b0"
      },
      "source": [
        "## Applications of LLMs\n",
        "While the power of LLMs live in their ability to adapt to various situations, there are handful of common applications that we'll reference several time during the course. For a given task, there's always the option to use an LLM out of the box (no prompting, fine tuning, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d6235a7",
      "metadata": {
        "id": "0d6235a7"
      },
      "source": [
        "#### **Zero-shot learning**\n",
        "Zero-shot learning refers to the ability of these language models to generate responses or perform tasks for which they have not been explicitly trained or provided with any examples. Zero-shot learning leverages the general knowledge and understanding of language encoded in the LLMs to generate contextually appropriate responses in unseen scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0248491",
      "metadata": {
        "id": "b0248491"
      },
      "source": [
        "Let's look at an example. We'll be leveraging the transformers library from Huggingface, namely the pipeline module which is a high level API that useful for getting started with language models quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6f0330d",
      "metadata": {
        "id": "e6f0330d",
        "outputId": "78790796-0744-4559-fbf8-14aa97a372ea",
        "colab": {
          "referenced_widgets": [
            "c4ab5ba6d05644b48999d1ecd67f2321",
            "0ad91ad4c23647d98a8bdf19973bf692",
            "bfb3451f4ce74b339e6dca1e4c5a4bba",
            "67d0eaedf1b948adab7b64384c7495ce",
            "5616e8886bad41cea0adf4507d0a4afd"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4ab5ba6d05644b48999d1ecd67f2321",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ad91ad4c23647d98a8bdf19973bf692",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfb3451f4ce74b339e6dca1e4c5a4bba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67d0eaedf1b948adab7b64384c7495ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5616e8886bad41cea0adf4507d0a4afd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "# Specify the bart large model to be used for zero shot classification\n",
        "classifier = pipeline(\"zero-shot-classification\",\n",
        "                      model=\"facebook/bart-large-mnli\",\n",
        "                     framework=\"pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2e11a7f",
      "metadata": {
        "id": "d2e11a7f",
        "outputId": "190bf4f5-06ae-4761-d8ac-353ba68089b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sequence': 'one day I will see the world',\n",
              " 'labels': ['travel', 'dancing', 'cooking'],\n",
              " 'scores': [0.9938652515411377, 0.0032737920992076397, 0.0028610059525817633]}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequence_to_classify = \"one day I will see the world\"\n",
        "candidate_labels = ['travel', 'cooking', 'dancing']\n",
        "classifier(sequence_to_classify, candidate_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b553d9fb",
      "metadata": {
        "id": "b553d9fb"
      },
      "source": [
        "#### **Few-shot learning**\n",
        "Similar to zero-shot learning, few-shot learning refers to the capability of these language models to generalize and learn from a limited number of examples or prompts for new tasks or classes. Unlike traditional machine learning algorithms that require a large amount of labeled data for training, few-shot learning enables LLMs to quickly adapt and generate responses based on a small set of examples.\n",
        "\n",
        "In few-shot learning, the LLM is provided with a small number of labeled examples or prompts for a specific task or class. These examples serve as a training set, allowing the LLM to learn the patterns and characteristics associated with that task or class. The model leverages its pre-existing knowledge and language understanding to generalize from these limited examples and generate responses or perform the desired task accurately.\n",
        "\n",
        "The few-shot learning approach often involves techniques such as meta-learning, where the LLM is trained to quickly adapt to new tasks or classes based on a few examples. These techniques aim to capture the underlying patterns and similarities between the examples and utilize that knowledge to make predictions or generate relevant text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "107ada57",
      "metadata": {
        "id": "107ada57"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\" For each tweet, classify the sentiment:\n",
        "\n",
        "Tweet: \"I hate spinach\"\n",
        "Sentiment: negative\n",
        "###\n",
        "Tweet: \"I love apples\"\n",
        "Sentiment: positive\n",
        "###\n",
        "Tweet: \"The sky is blue\"\n",
        "Sentiment: neutral\n",
        "###\n",
        "Tweet: \"Eating watermelon in the summer brings me immense joy\"\n",
        "Sentiment:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b93bb9aa",
      "metadata": {
        "id": "b93bb9aa",
        "outputId": "f8eb1af5-1f8b-46f6-af7b-8cc67f853cf6",
        "colab": {
          "referenced_widgets": [
            "b01290047e6942ac8fca39d2a87336c9",
            "8cfc1e5c090542e4ab77b23129a81863",
            "9f65c924cd3f4116abd8ec7ae954d859",
            "e52c9b799aa74243926d49a6f683e8f8",
            "1961de0c6dc14d0d98065ded8b382f3c",
            "ded62d00ad994d72ba8b722445247b8d"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b01290047e6942ac8fca39d2a87336c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cfc1e5c090542e4ab77b23129a81863",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/5.31G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f65c924cd3f4116abd8ec7ae954d859",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e52c9b799aa74243926d49a6f683e8f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1961de0c6dc14d0d98065ded8b382f3c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ded62d00ad994d72ba8b722445247b8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "few_shot_learning_pipeline = pipeline(task=\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\",\n",
        "                                      max_new_tokens=10, framework=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "395de6ec",
      "metadata": {
        "id": "395de6ec"
      },
      "outputs": [],
      "source": [
        "# Since the ### sign implies the end of our prompt, we'll need to specify that as our end of sequence token\n",
        "eos_token_id = few_shot_learning_pipeline.tokenizer.encode(\"###\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdd640ba",
      "metadata": {
        "id": "cdd640ba",
        "outputId": "4f36e350-2784-44b0-e27a-79858642fc05"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:21017 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " For each tweet, classify the sentiment:\n",
            "\n",
            "Tweet: \"I hate spinach\"\n",
            "Sentiment: negative \n",
            "###\n",
            "Tweet: \"I love apples\"\n",
            "Sentiment: positive \n",
            "###\n",
            "Tweet: \"The sky is blue\"\n",
            "Sentiment: neutral\n",
            "###\n",
            "Tweet: \"Eating watermelon in the summer brings me immense joy\"\n",
            "Sentiment: positive \n",
            "\n",
            "As the world struggles to cope\n"
          ]
        }
      ],
      "source": [
        "print(few_shot_learning_pipeline(prompt, eos_token_id=eos_token_id)[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2edec3a",
      "metadata": {
        "id": "c2edec3a"
      },
      "source": [
        "#### **Fine Tuning**\n",
        "\n",
        "Fine-tuning refers to the process of further training a pre-trained language model on a specific task or domain using a smaller, task-specific dataset. Fine-tuning allows LLMs to specialize and adapt their knowledge to better perform a specific task or improve performance in a particular domain.\n",
        "\n",
        "During fine-tuning, the pre-trained LLM, which has already learned general language patterns and semantics from a large corpus of data, is exposed to a new dataset that is specific to the target task or domain. This dataset is typically smaller in size and labeled with task-specific annotations or examples.\n",
        "\n",
        "The fine-tuning process involves updating the parameters of the LLM using the new task-specific data while preserving the knowledge acquired during pre-training. The model is trained to refine its understanding and generate more accurate outputs for the specific task or domain it is being fine-tuned for.\n",
        "\n",
        "By fine-tuning, LLMs can adapt their language generation capabilities to specific applications such as text completion, sentiment analysis, or named entity recognition. This process allows the LLM to leverage its pre-existing knowledge while aligning its performance with the requirements of the task or domain at hand.\n",
        "\n",
        "Fine-tuning is beneficial because it allows LLMs to quickly adapt to new tasks or domains without starting the training process from scratch. The pre-trained LLMs serve as a strong foundation, and fine-tuning facilitates the transfer of knowledge to specific tasks, leading to improved performance and efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "426f75d0",
      "metadata": {
        "id": "426f75d0"
      },
      "source": [
        "** Note that we'll discuss fine-tuning in great detail in lesson 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16061fc5",
      "metadata": {
        "id": "16061fc5"
      },
      "source": [
        "## Flow Chaining\n",
        "While these models can do some pretty incredible things with just the tasks they're trained on, the models can also be chained together to produce more complex pipelines. Flow chaining, in the context of LLMs, refers to the process of generating a sequence of outputs by iteratively feeding the model's previous output as input for the next step. It allows for the generation of longer and more coherent pieces of text, extending beyond a single prompt or query.\n",
        "\n",
        "In flow chaining, the initial input prompt or query is given to the LLM to generate an output. Instead of stopping at the initial response, the generated text is then appended or concatenated to the input as a continuation, forming a new prompt. This concatenated prompt is then passed back to the model for further generation. This process is repeated iteratively to generate longer pieces of text, building a coherent flow of information or narrative.\n",
        "\n",
        "Flow chaining can be beneficial in scenarios where a continuous and contextually consistent output is desired, such as story generation, dialogue systems, or document completion. By using the previous outputs as part of the input, the LLM can maintain consistency and coherence throughout the generated text.\n",
        "\n",
        "\n",
        "![flow_chaining](./images/flow_chaining.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa455847",
      "metadata": {
        "id": "aa455847"
      },
      "source": [
        "Consider the architecture above - say that this depicts the flow for a Q&A Bot. The goal of this application is to be able to answer questions about various documents. However, several models have token limits which means users can't just feed 5+ page documents into a Q&A model and expect a response. To combat this limitation, we can chain multiple models together. We can first use a summarization model, to summarized the documents then feed those summaries into the Q&A model to generate a result. This way we can provide a reasonable amount of context per document, without worrying about the model's token limit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82baf6b3",
      "metadata": {
        "id": "82baf6b3"
      },
      "source": [
        "Applications of flow chaining go far beyond this simple example. In lesson four we will dive deeper into chains, their capabilites and frameworks like [LangChain](https://github.com/hwchase17/langchain) that are built to support these larger LLM chains."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}